{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint, choice\n",
    "import re\n",
    "import regex\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/vec.csv', delimiter=',')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Векторизация категориальных признаков\n",
    "\n",
    "one_hot = pd.get_dummies(df['age'])\n",
    "df = df.drop('age',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "df.rename(columns = {1.0:'age1',2.0:'age2',3.0:'age3',}, inplace = True)\n",
    "\n",
    "one_hot = pd.get_dummies(df['trim'])\n",
    "df = df.drop('trim',axis = 1)\n",
    "df = df.join(one_hot) \n",
    "df.rename(columns = {1.0:'trim1',2.0:'trim2',3.0:'trim3',}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Векторизация текстов методом самообученного FastText\n",
    "\n",
    "df0=df.copy(deep=True)\n",
    "modelwv = FastText.load('models/fasttext')\n",
    "\n",
    "x1=[]\n",
    "df0['fir']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"fir\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=modelwv.wv[df['tokens'][i][j]]\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['fir'][i]=x1[i]\n",
    "    \n",
    "x=[]\n",
    "df0['sec']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"sec\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=modelwv.wv[df['tokens'][i][j]]\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x.append(t)\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['sec'][i]=x[i]\n",
    "    \n",
    "x1=[]\n",
    "df0['thr']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"thr\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=modelwv.wv[df['tokens'][i][j]]\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['thr'][i]=x1[i]\n",
    "    \n",
    "x=[]\n",
    "df0['com']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"com\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=modelwv.wv[df['tokens'][i][j]]\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x.append(t)\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['com'][i]=x[i]\n",
    "\n",
    "x1=[]\n",
    "df0['diagnos']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"diagnos\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=modelwv.wv[df['tokens'][i][j]]\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['diagnos'][i]=x1[i]\n",
    "    \n",
    "df0.to_pickle(\"data/FTv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Векторизация текстов методом предобученного FastText\n",
    "\n",
    "df0=df.copy(deep=True)\n",
    "fasttext.util.download_model('ru', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.ru.300.bin')\n",
    "\n",
    "x1=[]\n",
    "df0['fir']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"fir\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=ft.get_word_vector(df['tokens'][i][j])\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=ft.get_word_vector('нет')\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['fir'][i]=x1[i]\n",
    "    \n",
    "x=[]\n",
    "df0['sec']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"sec\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=ft.get_word_vector(df['tokens'][i][j])\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=ft.get_word_vector('нет')\n",
    "    if q.shape==(300,):\n",
    "        x.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x.append(t)\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['sec'][i]=x[i]\n",
    "    \n",
    "x1=[]\n",
    "df0['thr']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"thr\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=ft.get_word_vector(df['tokens'][i][j])\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=ft.get_word_vector('нет')\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['thr'][i]=x1[i]\n",
    "    \n",
    "x=[]\n",
    "df0['com']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"com\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=ft.get_word_vector(df['tokens'][i][j])\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=ft.get_word_vector('нет')\n",
    "    if q.shape==(300,):\n",
    "        x.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x.append(t)\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['com'][i]=x[i]\n",
    "\n",
    "x1=[]\n",
    "df0['diagnos']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"diagnos\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        b=ft.get_word_vector(df['tokens'][i][j])\n",
    "        if type(q)==int:\n",
    "            q=b\n",
    "        else:\n",
    "            q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=ft.get_word_vector('нет')\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['diagnos'][i]=x1[i]\n",
    "    \n",
    "df0.to_pickle(\"data/FT.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Векторизация текстов методом самообученного Word2Vec\n",
    "\n",
    "df0=df.copy(deep=True)\n",
    "modelwv = Word2Vec.load('models/model5_300')\n",
    "\n",
    "x1=[]\n",
    "df0['fir']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"fir\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        if df['tokens'][i][j] in (modelwv.wv):\n",
    "            b=modelwv.wv[df['tokens'][i][j]]\n",
    "            if type(q)==int:\n",
    "                q=b\n",
    "            else:\n",
    "                q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['fir'][i]=x1[i]\n",
    "    \n",
    "x=[]\n",
    "df0['sec']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"sec\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        if df['tokens'][i][j] in (modelwv.wv):\n",
    "            b=modelwv.wv[df['tokens'][i][j]]\n",
    "            if type(q)==int:\n",
    "                q=b\n",
    "            else:\n",
    "                q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x.append(t)\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['sec'][i]=x[i]\n",
    "    \n",
    "x1=[]\n",
    "df0['thr']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"thr\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        if df['tokens'][i][j] in (modelwv.wv):\n",
    "            b=modelwv.wv[df['tokens'][i][j]]\n",
    "            if type(q)==int:\n",
    "                q=b\n",
    "            else:\n",
    "                q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['thr'][i]=x1[i]\n",
    "    \n",
    "x=[]\n",
    "df0['com']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"com\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        if df['tokens'][i][j] in (modelwv.wv):\n",
    "            b=modelwv.wv[df['tokens'][i][j]]\n",
    "            if type(q)==int:\n",
    "                q=b\n",
    "            else:\n",
    "                q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x.append(t)\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['com'][i]=x[i]\n",
    "\n",
    "x1=[]\n",
    "df0['diagnos']=''\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"diagnos\"].apply(tokenizer.tokenize)\n",
    "for i in range (len(df)):  \n",
    "    q=0\n",
    "    for j in range (len(df['tokens'][i])):\n",
    "        if df['tokens'][i][j] in (modelwv.wv):\n",
    "            b=modelwv.wv[df['tokens'][i][j]]\n",
    "            if type(q)==int:\n",
    "                q=b\n",
    "            else:\n",
    "                q=np.vstack((q, b))\n",
    "    if type(q)==int:\n",
    "        q=modelwv.wv['нет']\n",
    "    if q.shape==(300,):\n",
    "        x1.append(t)\n",
    "    else:\n",
    "        t=q.sum(axis=0)/q.shape[0]\n",
    "        x1.append(t)\n",
    "x1 = np.array(x1)\n",
    "for i in range(len(df0)):\n",
    "    df0['diagnos'][i]=x1[i]\n",
    "    \n",
    "df0.to_pickle(\"data/w2v.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Векторизация текстов методом BertEmbedings\n",
    "\n",
    "df0=df.copy(deep=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', do_basic_tokenize=False)\n",
    "emb = torch.nn.Embedding(tokenizer.vocab_size, 768)\n",
    "emb.load_state_dict(torch.load('models/bert_emb.bin'))\n",
    "\n",
    "df0['com']=''\n",
    "x = []\n",
    "for i in range (len(df)):\n",
    "    if df['com'][i]=='':\n",
    "        df['com'][i]='нет'\n",
    "    t= tokenizer.tokenize(df['com'][i])\n",
    "    with torch.no_grad():\n",
    "        t_v_1 = emb(torch.tensor(tokenizer.convert_tokens_to_ids(t))).numpy()\n",
    "    x.append(t_v_1.sum(axis=0)/t_v_1.shape[0])\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['com'][i]=x[i]\n",
    "\n",
    "df0['diagnos']=''\n",
    "x = []\n",
    "for i in range (len(df)):\n",
    "    if df['diagnos'][i]=='':\n",
    "        df['diagnos'][i]='delete'\n",
    "    t= tokenizer.tokenize(df['diagnos'][i])\n",
    "    with torch.no_grad():\n",
    "        t_v_1 = emb(torch.tensor(tokenizer.convert_tokens_to_ids(t))).numpy()\n",
    "    x.append(t_v_1.sum(axis=0)/t_v_1.shape[0])\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['diagnos'][i]=x[i]\n",
    "    \n",
    "df0['fir']=''\n",
    "x = []\n",
    "for i in range (len(df)):\n",
    "    if ((df['fir'][i]=='') or (df['fir'][i]=='    ')):\n",
    "        df['fir'][i]='нет'\n",
    "    t= tokenizer.tokenize(df['fir'][i])\n",
    "    with torch.no_grad():\n",
    "        #print(i)\n",
    "        t_v_1 = emb(torch.tensor(tokenizer.convert_tokens_to_ids(t))).numpy()\n",
    "    x.append(t_v_1.sum(axis=0)/t_v_1.shape[0])\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['fir'][i]=x[i]\n",
    "    \n",
    "df0['sec']=''\n",
    "x = []\n",
    "for i in range (len(df)):\n",
    "    if ((df['sec'][i]=='') or (df['sec'][i]=='    ')or (df['sec'][i]=='  ')):\n",
    "        df['sec'][i]='нет'\n",
    "    t= tokenizer.tokenize(df['sec'][i])\n",
    "    with torch.no_grad():\n",
    "        t_v_1 = emb(torch.tensor(tokenizer.convert_tokens_to_ids(t))).numpy()\n",
    "    x.append(t_v_1.sum(axis=0)/t_v_1.shape[0])\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['sec'][i]=x[i]\n",
    "    \n",
    "df0['thr']=''\n",
    "x = []\n",
    "for i in range (len(df)):\n",
    "    if ((df['thr'][i]=='')or (df['thr'][i]=='   ')):\n",
    "        df['thr'][i]='нет'\n",
    "    t= tokenizer.tokenize(df['thr'][i])\n",
    "    with torch.no_grad():\n",
    "        t_v_1 = emb(torch.tensor(tokenizer.convert_tokens_to_ids(t))).numpy()\n",
    "    x.append(t_v_1.sum(axis=0)/t_v_1.shape[0])\n",
    "x = np.array(x)\n",
    "for i in range(len(df0)):\n",
    "    df0['thr'][i]=x[i]\n",
    "    \n",
    "df0.to_pickle(\"data/Bert.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
